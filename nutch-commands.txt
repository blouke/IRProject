Nutch 1.x Crawling Steps

bin/nutch inject crawl/crawldb urls

# FIRST ROUND
bin/nutch generate crawl/crawldb crawl/segments
export SEGMENT=`ls -d crawl/segments/2* | tail -1`
bin/nutch fetch $SEGMENT
bin/nutch parse $SEGMENT
bin/nutch updatedb crawl/crawldb $SEGMENT

# SECOND ROUND
bin/nutch generate crawl/crawldb crawl/segments
export SEGMENT=`ls -d crawl/segments/2* | tail -1`
bin/nutch fetch $SEGMENT
bin/nutch parse $SEGMENT
bin/nutch updatedb crawl/crawldb $SEGMENT




Check status of crawldb
bin/nutch readdb crawl/crawldb -stats


#dump only parsed text of each url from all segments.
bin/nutch readseg -dump crawl/segments/* crawl/dump -nocontent -nofetch -nogenerate -noparse -noparsedata


NOTES
set db.ignore.external.links to "true", and inject seeds from the domains you wish to crawl (these seeds must link to all pages you wish to crawl, directly or indirectly). Doing this will let the crawl go through only these domains without leaving to start crawling external links

The crawl tool has a default limitation of 100 outlinks of one page that are being fetched. To overcome this limitation change the db.max.outlinks.per.page property to a higher value or simply -1 (unlimited).


set fetcher.max.crawl.delay to 5 seconds to ignore pages beyond this much delay.

